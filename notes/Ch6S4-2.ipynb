{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-activity",
   "metadata": {},
   "source": [
    "# The Gauss-Newton method and Gradient descent\n",
    "\n",
    "The golden ratio method cannot be be generalized to functions of several variables.\n",
    "\n",
    "## Gauss-Newton method\n",
    "\n",
    "We can find a maximum or minimum of a function $f(x)$ by differentiating it and setting the result equal to zero. This means that minima and maxima are nothing other than roots of the derivative function $f'(x)$. We can use the Newton-Raphson method\n",
    "\n",
    "$$ x' = x - \\frac{f'(x)}{f''(x)} $$\n",
    "\n",
    "We haver derived the fundamental formula for the Gauss-Newton method. This can be generalized to find maxima and minima of functions of more than one variable.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "If we can calculate only the first derivative of a function, then we can still do an approximate version of the Gauss-Newton method by writing\n",
    "\n",
    "$$ x' \\approx x - \\gamma f'(x) $$\n",
    "\n",
    "where $\\gamma $ is a constant value that represents a rough guess at $1/f''(x)$. This constant does not have to be very accurate for the method to work. Any value of roughly the right order of magnitude will allow  you to find your minimum or maximum in a reasonable number of steps. This method is called Gradient descent.\n",
    "\n",
    "For positive values of $\\gamma$ the method will move \"downhill\" from $x$ to $x'$ and converge towards a minimum of the function. For negative $\\gamma$ it will converge towards a maximum. The magnitude of $\\gamma$ controls the rate of convergence. If $\\gamma $ is larger then we will converge more quickly in general, but if it is too large then we may overshoot and fail to find the maximum or minimum we are looking for. In general, the value should be about equal to $1/f''(x)$, but if we cannot calculate the second derivative then we must guess a value for $\\gamma$ instead. Trial and error is often the quickest way to an answer.\n",
    "\n",
    "There are, however, many cases where we cannot calculate even the first derivative of a function. In such cases we can estimate it. We start with two points $x_1$ and $x_2$, and use them to calculate an approximation to the slope $f'(x)$ and\n",
    "\n",
    "$$ x_3 \\approx x_2 - \\gamma \\frac{f(x_2) - f(x_1)}{x_2 - x_1} $$\n",
    "\n",
    "For suitable choices of $\\gamma$, this expression boasts rates of convergence similar to Newton's method, while requiring us to calculate only the value of the function $f(x)$, and not its derivatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
